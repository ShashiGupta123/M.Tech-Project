{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPv5w9ZMxKarQHhJ47sZEHi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShashiGupta123/M.Tech-Project/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IXDWwtePkmTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4c6300-ebd8-416c-de50-2af3626ec4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import seaborn as sns\n",
        "import matplotlib.cm as cm\n",
        "import itertools\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n"
      ],
      "metadata": {
        "id": "x_1ZIfdzm-c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Mtech_Project/Dataset/train.csv/train.csv')"
      ],
      "metadata": {
        "id": "DGE83my7nB-u",
        "outputId": "26f6790d-82bb-4191-fbba-8bce6ede9cb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fd53056a4fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Mtech_Project/Dataset/train.csv/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Mtech_Project/Dataset/train.csv/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "9sAV0KX0nPjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "cPCVGikcnVCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "zDPFbW9qncRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "TQCLC18Cngzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "w_LGiPXbno7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "rI48z6S8nxgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding all the rows where the sum of labels is zero i.e the comment is a Clean comment\n",
        "rowsums=df.iloc[:,2:].sum(axis=1)\n",
        "df['clean']=(rowsums==0)\n",
        "df['clean'].sum()\n"
      ],
      "metadata": {
        "id": "8DRM3UX2n49V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total no.of toxic comments\n",
        "len(df[df['toxic']==1])\n"
      ],
      "metadata": {
        "id": "pPXspXfln-y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "XyKB5uBRoDxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Using seaborn and matplotlib to visualize the count of different categories of toxicity of comments.\n",
        "# \n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "# Using seaborn and matplotlib to visualize the count of different categories of toxicity of comments\n",
        "\n",
        "colors_list = [\"green\", \"purple\",\"pine green\",\"red\", \"brown\",\"yellow\", \"blue\"]\n",
        "\n",
        "palette= sns.xkcd_palette(colors_list)\n",
        "\n",
        "x=df.iloc[:,2:].sum()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "# x.index has all the toxicity labels and x.values has their respective count\n",
        "ax= sns.barplot(x.index, x.values,palette=palette)\n",
        "plt.title(\"Class\")\n",
        "plt.xlabel('Toxicity Type', fontsize = 12)\n",
        "plt.ylabel('Occurrences', fontsize=12)\n",
        "rects = ax.patches\n",
        "labels = x.values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 10, label, \n",
        "            ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XLUd4HumoKT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **The graph shows us that the dataset is highly imbalanced as more than 1.4lac comments are categorized as clean**\n",
        "\n",
        "# ### Taking an insight of the length of the comments in the dataset.\n",
        "comment = df['comment_text']\n",
        "for i in range(5):\n",
        "    print(i,\"- \" + comment[i] + \"\\n Length -\" ,len(comment[i]))"
      ],
      "metadata": {
        "id": "VWQSTOHIoVX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ***The length of the comments looks to be quite large, so we'll visualize some more info about the comments.***\n",
        "\n",
        "# creating a numpy array of the length of each comment in the dataset.\n",
        "x = np.array([len(comment[i]) for i in range(comment.shape[0])])\n",
        "\n"
      ],
      "metadata": {
        "id": "RycOlbDwohA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\"\"The maximum length of comment is:{} \n",
        "        \\nThe minimum length of the comment is:{} \n",
        "        \\nAnd the average length of a comment is: {}\"\"\".format(x.max(),x.min(),x.mean()))\n"
      ],
      "metadata": {
        "id": "vSGs7xJWom91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('The average length of comment is : 394.073' )\n",
        "bins = [1,200,400,600,800,1000,1200,1400]\n",
        "plt.hist(x, bins=bins, color = 'Blue')\n",
        "plt.xlabel('Length of comments')\n",
        "plt.ylabel('Number of comments')       \n",
        "plt.axis([0, 1400, 0, 90000])\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8TMMTDOaosYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It is visible that length of most of the comments(Approx 80,000) lies in the range of 0-200 and around 40,000 lie in between 200-400\n",
        "#Now we will try to find the count of different toxicity of comments in each bin\n",
        "label = df[['toxic', 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate']]\n",
        "print(label.head(10))\n",
        "label = label.values\n"
      ],
      "metadata": {
        "id": "_oYhNvaMozN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label.shape"
      ],
      "metadata": {
        "id": "IrBQp2EvpB6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a zero matrix of shape (159571,6)\n",
        "y = np.zeros(label.shape)\n",
        "for i in range(label.shape[0]):\n",
        "    l = len(comment[i])\n",
        "    if label[i][0] :\n",
        "        y[i][0] = l\n",
        "    if label[i][1] :\n",
        "        y[i][1] = l\n",
        "    if label[i][2] :\n",
        "        y[i][2] = l\n",
        "    if label[i][3] :\n",
        "        y[i][3] = l\n",
        "    if label[i][4] :\n",
        "        y[i][4] = l\n",
        "    if label[i][5] :\n",
        "        y[i][5] = l\n",
        "\n",
        "label_plot = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "color = ['blue','green','red','yellow','orange','chartreuse']    \n",
        "plt.figure(figsize = (10,8))\n",
        "plt.hist(y,bins = bins,label = label_plot,color = color)\n",
        "plt.axis([0, 1400, 0, 12000])\n",
        "plt.xlabel('Length of comments', fontsize = 14)\n",
        "plt.ylabel('Number of comments', fontsize = 14) \n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eBmQGkbFE9ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Remove excessive length comments\n",
        "\n",
        "# In[20]\n",
        "# creating a list of comments with less than 400 length of words.\n",
        "trim_comments = [comment[i] for i in range(comment.shape[0]) if len(comment[i])<=400 ]\n",
        "\n",
        "# creating corresponding labels for those comments\n",
        "my_labels = np.array([label[i] for i in range(comment.shape[0]) if len(comment[i])<=400 ])\n"
      ],
      "metadata": {
        "id": "nBgk6x5sFnkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In[21]:\n",
        "\n",
        "\n",
        "my_labels[:10, :]"
      ],
      "metadata": {
        "id": "Ji6k3RolFraT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(trim_comments))\n",
        "print(len(my_labels))\n",
        "print(\"Thus number of removed comments = {}\".format(159571-115910))\n"
      ],
      "metadata": {
        "id": "AC8NZ6GhF0jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# *So now we are left with only 115910 comments whose length is less than 400*\n",
        "\n",
        "# In[23]:\n",
        "\n",
        "\n",
        "print(len(trim_comments))\n",
        "print(my_labels.shape)"
      ],
      "metadata": {
        "id": "t2tl1FLkF2n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Preprocessing of the comments.\n",
        "# \n",
        "#  *This involves :*\n",
        "# \n",
        "# * Removing punctuation and special characters from the comments\n",
        "# * Removing **Stop words** i.e removing such words that don't add to the meaning of the sentence.\n",
        "# * Stemming and Lemmatizing the words.\n",
        "# * Applying count Vectoriser\n",
        "# * Splitting dataset into training and testing.\n",
        "# \n",
        "\n",
        "# In[24]:\n",
        "\n",
        "\n",
        "# Punctuation removal\n",
        "\n",
        "import string\n",
        "print(string.punctuation)\n",
        "punctuation_edit = string.punctuation.replace('\\'','') +\"0123456789\"\n",
        "print (punctuation_edit)\n",
        "outtab = \"                                         \"\n",
        "trantab = str.maketrans(punctuation_edit, outtab)\n",
        "\n"
      ],
      "metadata": {
        "id": "40IWEOvJF7yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Adding alphabets to the set\n",
        "for i in range(ord('a'),ord('z')+1):\n",
        "    stop_words.add(chr(i))\n",
        "print(stop_words)\n"
      ],
      "metadata": {
        "id": "q_zaM4JUF_zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming and Lemmatizing\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "yoyW_RFLF_2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "for i in range(len(trim_comments)):\n",
        "    trim_comments[i] = trim_comments[i].lower().translate(trantab)\n",
        "    word_list = []\n",
        "    for word in trim_comments[i].split():\n",
        "        if not word in stop_words:\n",
        "            word_list.append(stemmer.stem(lemmatizer.lemmatize(word,pos=\"v\")))\n",
        "    trim_comments[i]  = \" \".join(word_list)"
      ],
      "metadata": {
        "id": "dWFqyTnIGFq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comments after stop words removal, stemming and lemmatizing.\n",
        "for i in range(5):\n",
        "    print(trim_comments[i],\"\\n\")"
      ],
      "metadata": {
        "id": "4Q1ibr9_GJzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GyNgYzWgW_Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying count vectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        " \n",
        "#create object supplying our custom stop words\n",
        "count_vector = CountVectorizer(stop_words=stop_words)\n",
        "#fitting it to converts comments into bag of words format\n",
        "tf = count_vector.fit_transform(trim_comments[:20000]).toarray()"
      ],
      "metadata": {
        "id": "Ca5UhcV3GNaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RU9Ln2z_cQsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.shape"
      ],
      "metadata": {
        "id": "N26PYJthGSLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle(matrix, target, test_proportion):\n",
        "    ratio = int(matrix.shape[0]/test_proportion)\n",
        "    X_train = matrix[ratio:,:]\n",
        "    X_test =  matrix[:ratio,:]\n",
        "    Y_train = target[ratio:,:]\n",
        "    Y_test =  target[:ratio,:]\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = shuffle(tf, my_labels[:20000],3)\n",
        "\n",
        "print(X_test.shape)\n",
        "print(X_train.shape)\n"
      ],
      "metadata": {
        "id": "zi3ahnT2GVV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def evaluate_score(Y_test,predict): \n",
        "    loss = hamming_loss(Y_test,predict)\n",
        "    print(\"Hamming_loss : {}\".format(loss*100))\n",
        "    accuracy = accuracy_score(Y_test,predict)\n",
        "    print(\"Accuracy : {}\".format(accuracy*100))\n",
        "    try : \n",
        "        loss = log_loss(Y_test,predict)\n",
        "    except :\n",
        "        loss = log_loss(Y_test,predict.toarray())\n",
        "    print(\"Log_loss : {}\".format(loss))\n"
      ],
      "metadata": {
        "id": "vpmTKPi0GpJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n"
      ],
      "metadata": {
        "id": "qEuIFJY4Qld2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = []\n",
        "for i in range(6):\n",
        "    clf.append(MultinomialNB())\n",
        "    clf[i].fit(X_train,Y_train[:,i])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QpSl7ansQvE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict list contains the predictions, it is transposed later to get the proper shape\n",
        "predict = []\n",
        "for i in range(6):\n",
        "    predict.append(clf[i].predict(X_test))\n",
        "\n",
        "predict = np.asarray(np.transpose(predict))\n",
        "print(predict.shape)\n"
      ],
      "metadata": {
        "id": "E-juMQjhRLxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_score(Y_test,predict)"
      ],
      "metadata": {
        "id": "R8dNIuVRRQDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-multilearn"
      ],
      "metadata": {
        "id": "9yVZN5fdRV8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 2. BR Method with SVM classifier (from scikit-multilearn)\n",
        "# In[35]:\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.svm import SVC\n",
        "classifier = BinaryRelevance(classifier = SVC(), require_dense = [False, True])\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "9gCT8bUD6s8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predictions\n",
        "predictions = classifier.predict(X_test)\n"
      ],
      "metadata": {
        "id": "jA-T80GpDAYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate scores\n",
        "evaluate_score(Y_test,predictions)\n",
        "\n"
      ],
      "metadata": {
        "id": "tuxkW-jADKkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "#create and fit classifiers\n",
        "clf = []\n",
        "for i in range(6):\n",
        "    clf.append(GaussianNB())\n",
        "    clf[i].fit(X_train,Y_train[:,i])\n",
        "\n"
      ],
      "metadata": {
        "id": "KX0imiZsDTM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predictions\n",
        "predict = []\n",
        "for ix in range(6):\n",
        "    predict.append(clf[ix].predict(X_test))\n"
      ],
      "metadata": {
        "id": "6C3-cfZwDfhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate scores\n",
        "predict = np.asarray(np.transpose(predict))\n",
        "evaluate_score(Y_test,predict)\n",
        "\n"
      ],
      "metadata": {
        "id": "5a3WbN0DDmuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = ['BR-MultnomialNB','BR-SVC','BR-GaussianNB']\n",
        "y = [3.65,4.36,20.74]\n",
        "colors = itertools.cycle(['b', 'g', 'r'])\n",
        "plt.figure(figsize= (8,6))\n",
        "plt.ylabel('Hamming-Loss')\n",
        "plt.xlabel('Model-details')\n",
        "plt.xticks(rotation=90)\n",
        "for i in range(len(y)):\n",
        "    plt.bar(x[i], y[i], color=next(colors))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YFJ2lDSrDtBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = ['BR-MultNB','BR-SVC','BR-GausNB']\n",
        "y = [1.97,0.46,1.422]\n",
        "colors = itertools.cycle(['b', 'g', 'r'])\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.ylabel('Log-Loss')\n",
        "plt.xlabel('Model-details')\n",
        "plt.xticks(rotation=90)\n",
        "for i in range(len(y)):\n",
        "    plt.bar(x[i], y[i], color=next(colors))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g9cyVG1LEAlA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}